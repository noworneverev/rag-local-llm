{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "# model_path = './models/mistral-7b-instruct-v0.2.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    # model_path='./models/llama-2-13b-chat.Q5_0.gguf',\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,    \n",
    "    completion_to_prompt=completion_to_prompt,        \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here is a poem about fast cars:\n",
      "Racing down the highway, wind in my hair\n",
      "The engine purring smoothly, without a care\n",
      "Fast and free, like a bird in flight\n",
      "The thrill of speed, a feeling so bright\n",
      "\n",
      "A sleek machine, built for speed and grace\n",
      "Cutting through the air, with precision and pace\n",
      "The roar of the engine, a symphony to hear\n",
      "As I drive fast, my heart full of cheer\n",
      "\n",
      "The world outside, a blur in my view\n",
      "But the thrill of the ride, forever true\n",
      "Fast cars, a dream come true\n",
      "A feeling that's mine, and yours too."
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "\n",
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "text_embedding = embed_model.get_text_embedding(\"hello world\")\n",
    "print(len(text_embedding))\n",
    "\n",
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to print out the response\n",
    "def query(query_str):\n",
    "    streaming_response = query_engine.query(query_str)\n",
    "    streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on the given context, here's an answer to your query:\n",
      "To get started on a personal project in AI, you can follow these steps:\n",
      "1. Identify your interests and goals: Think about what areas of AI interest you the most, such as natural language processing, computer vision, or machine learning. Also, consider what you want to achieve through this project, whether it's to develop a new skill, build a portfolio, or solve a real-world problem.\n",
      "2. Research and brainstorm: Once you have a clear idea of your interests and goals, start researching the field and identifying potential projects. Read articles, watch videos, and engage in online forums to learn about the latest developments and trends in AI. Brainstorm ideas based on your research, and write them down.\n",
      "3. Start small: Don't feel overwhelmed by trying to build a complex project from the start. Begin with something simple that you can complete quickly, such as building a chatbot or creating a machine learning model for image classification. This will help you gain confidence and develop your skills.\n",
      "4. Join existing projects: Look for open-source AI"
     ]
    }
   ],
   "source": [
    "# answer retrieved from eBook-How-to-Build-a-Career-in-AI.pdf\n",
    "query(\"how do I get started on a personal project in AI?\")\n",
    "# query(\"How do I build a portfolio of AI projects?\")\n",
    "# query(\"Summarize the book in 500 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity is 8.7%."
     ]
    }
   ],
   "source": [
    "# answer retrieved from annualreport.pdf\n",
    "query(\"what was the FY2022 return on equity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To make Pineapple Chicken, you will need the following ingredients:\n",
      "* 1 lb boneless, skinless chicken breasts\n",
      "* 1 cup pineapple juice\n",
      "* 1/4 cup soy sauce\n",
      "* 2 tbsp vegetable oil\n",
      "* 2 cloves garlic, minced\n",
      "* 1 tsp grated ginger\n",
      "* 1/4 cup chopped green onions (optional)\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. Heat the oil in a large skillet or wok over medium-high heat. Add the chicken and cook until browned on all sides, about 5 minutes. Remove the chicken from the skillet and set aside.\n",
      "2. In the same skillet, add the pineapple juice, soy sauce, garlic, ginger, and green onions (if using). Stir to combine and bring to a simmer.\n",
      "3. Add the cooked chicken back to the skillet and stir to coat with the sauce. Cook for an additional 2-3 minutes, until the chicken is fully coated and heated through."
     ]
    }
   ],
   "source": [
    "# answer retrieved from recipes.pdf\n",
    "query(\"How to make Pineapple Chicken?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
