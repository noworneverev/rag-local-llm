{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from config import MODEL_PATH\n",
    "\n",
    "# model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "# model_path = './models/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n",
    "# model_url = 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,    \n",
    "    completion_to_prompt=completion_to_prompt,        \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here is a poem about fast cars:\n",
      "Racing down the highway, wind in my hair\n",
      "The engine purring smoothly, without a care\n",
      "Fast as lightning, feeling alive\n",
      "The rush of adrenaline, as I drive\n",
      "\n",
      "The thrill of speed, the joy it brings\n",
      "A feeling of freedom, as the miles sing\n",
      "With every curve and straightaway\n",
      "I feel alive, as the world fades away\n",
      "\n",
      "The roar of the engine, a symphony in my ears\n",
      "As I push the pedal to the floor, without any fears\n",
      "Fast cars, oh how they excite\n",
      "A feeling of power, as I take flight\n",
      "\n",
      "So here I'll stay, on this road of speed\n",
      "With the wind at my back, and my heart on a leash\n",
      "For there's no thrill like the one I feel\n",
      "When I'm driving fast, with my heart in a whirl."
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "\n",
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "text_embedding = embed_model.get_text_embedding(\"hello world\")\n",
    "print(len(text_embedding))\n",
    "\n",
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# # load documents\n",
    "# documents = SimpleDirectoryReader(\n",
    "#     input_files=[\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to print out the response\n",
    "def query(query_str):\n",
    "    streaming_response = query_engine.query(query_str)\n",
    "    streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on the given context, here's an answer to your query:\n",
      "To get started on a personal project in AI, there are several steps you can take:\n",
      "1. Identify your interests and goals: Think about what areas of AI interest you the most and what you want to achieve through your project. This will help you focus on a specific area and give you direction as you work on your project.\n",
      "2. Research and learn: Before starting any project, it's essential to learn about the basics of AI and its applications. Read books, articles, and online resources to gain a good understanding of the field. This will help you identify potential project ideas and give you the knowledge you need to execute them successfully.\n",
      "3. Find a problem to solve: Look for a problem in your daily life or in an area that interests you, and think about how AI could be used to solve it. This will give you a clear objective for your project and help you stay motivated throughout the development process.\n",
      "4. Start small: Don't try to tackle a complex project right from the start. Begin with something simple and gradually build up to more challenging projects as your skills improve. This will help"
     ]
    }
   ],
   "source": [
    "# answer retrieved from eBook-How-to-Build-a-Career-in-AI.pdf\n",
    "query(\"how do I get started on a personal project in AI?\")\n",
    "# query(\"How do I build a portfolio of AI projects?\")\n",
    "# query(\"Summarize the book in 500 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I'm just an AI model, I don't have access to real-time or historical financial data, nor can I provide investment advice. The information you are seeking is likely contained in a specific financial report or document, and I cannot provide that information without proper authorization or context.\n",
      "However, I can suggest that you consult the relevant financial reports or documents from the company in question to obtain the information you are looking for. These reports typically contain detailed financial data, including returns on equity, revenue, expenses, and other important metrics.\n",
      "Additionally, it's important to keep in mind that past performance is not always indicative of future results, and investing in the stock market involves risk. It's essential to conduct thorough research and analysis before making any investment decisions."
     ]
    }
   ],
   "source": [
    "# answer retrieved from annualreport.pdf\n",
    "query(\"what was the FY2022 return on equity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To make Pineapple Chicken, you will need the following ingredients:\n",
      "* 1 can of pineapple chunks (15 oz)\n",
      "* 2 cups of frozen broccoli (or stir-fry vegetable mix), thawed\n",
      "* 1/4 cup of pineapple juice, reserved from the canned pineapple\n",
      "* 1/4 tsp of garlic powder\n",
      "* 1/4 tsp of crushed red pepper (optional, for those who like a little heat)\n",
      "* 2 cups of diced cooked chicken (or 2 (10 oz) cans of chicken breast, drained and flaked)\n",
      "* 1/4 cup of sesame oil or canola oil\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. Heat the oil in a large skillet over medium-high heat. Add all ingredients EXCEPT the pineapple and chicken. Cook and stir until heated through, about 5-6 minutes.\n",
      "2. Add the pineapple and chicken; cook for another 2-3 minutes. Serve"
     ]
    }
   ],
   "source": [
    "# answer retrieved from recipes.pdf\n",
    "query(\"How to make Pineapple Chicken?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Window-sentence retrieval setup (Advanced retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=llm,\n",
    "    save_dir=\"./sentence_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on the given context, here's how you can build a portfolio of AI projects:\n",
      "1. Start small: As mentioned in the context, don't worry about starting too small. Begin with simple projects that demonstrate your understanding of AI concepts and techniques. This will help you gain confidence and build momentum for more complex projects.\n",
      "2. Focus on practical applications: While it's important to have a solid theoretical foundation in AI, it's equally crucial to show how your skills can be applied in real-world scenarios. Choose projects that demonstrate practical applications of AI, such as image classification, natural language processing, or predictive modeling.\n",
      "3. Show progression: As you build your portfolio, aim to show a clear progression in complexity and scope. This will help potential employers or mentors understand how you've grown and developed over time.\n",
      "4. Communicate your thinking: As mentioned in the context, communication is key when building a portfolio of AI projects. Make sure to explain your thought process and reasoning behind each project, so that others can understand the value and potential applications of your work.\n",
      "5. Seek feedback: Don't be afraid\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I build a portfolio of AI projects?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity (ROE) for Macquarie Group Limited is 18.7%.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what was the FY2022 return on equity?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you for providing the context information! Based on the instructions given, here is how to make Pineapple Chicken:\n",
      "Step 1: Heat oil in a large skillet over medium-high heat. Add all ingredients EXCEPT pineapple and chicken. Cook and stir until heated through, about 5-6 minutes.\n",
      "Step 2: Add pineapple and chicken to the skillet. Cook for another 2 minutes.\n",
      "Serve over instant brown rice or whole wheat pasta.\n",
      "Note: Thaw frozen vegetables in the microwave or by holding the package under cold running water for several minutes.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to make Pineapple Chicken?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Auto-merging retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.node_parser import HierarchicalNodeParser\n",
    "from llama_index.node_parser import get_leaf_nodes\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    merging_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=merging_context,\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    \n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank], service_context=automerging_index.service_context\n",
    "    )\n",
    "    \n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "index = build_automerging_index(\n",
    "    [document],\n",
    "    llm=llm,\n",
    "    save_dir=\"./merging_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_automerging_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building a portfolio of AI projects is an essential step in your career growth as it showcases your skills and progress over time. Here are some steps to help you build a strong portfolio:\n",
      "1. Start small: Don't worry about starting too small. Begin with simple projects that demonstrate your understanding of AI concepts. This will help you gain experience and build confidence in your abilities.\n",
      "2. Identify areas of interest: Determine what areas of AI interest you the most, such as machine learning, natural language processing, or computer vision. Focus on projects that align with these interests to showcase your skills and passion.\n",
      "3. Choose a project scope: Select projects that have a clear objective and scope. This will help you measure your progress and demonstrate your capabilities to potential employers.\n",
      "4. Show progression: Build a portfolio of projects that demonstrates your skill progression over time. Start with simple projects and gradually move on to more complex ones, showcasing your ability to learn and grow.\n",
      "5. Document your process: Keep detailed documentation of your project development process, including any challenges you faced and how you overcame them. This will help potential employers understand your thought process\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I build a portfolio of AI projects?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity for Macquarie Group is 18.7%.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what was the FY2022 return on equity?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 381e7c21-cda5-4566-a62e-f287b2ac72b9.\n",
      "...tir\ten.\t\ts\ttext: Add\t\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To make Pineapple Chicken, you will need the following ingredients:\n",
      "* 2 tsp canola or sesame oil\n",
      "* 1 (10-oz) package frozen broccoli (or stir-fry vegetable mix)\n",
      "* 1/4 tsp garlic powder\n",
      "* 1/4 tsp crushed red pepper (optional â€“ use if you like it hot!)\n",
      "* 1 cup coconut milk\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. Add all ingredients except pineapple and chicken to a pot or wok and stir-fry until heated through, about 5-6 minutes.\n",
      "2. Add pineapple and chicken to the pot and cook for another 2 minutes.\n",
      "3. Stir in coconut milk and simmer for about 10 minutes or until the chicken is cooked through.\n",
      "4. Scatter over the chilli and serve with rice or whole wheat pasta.\n",
      "Note: If using frozen pineapple, thaw it in the microwave or by holding the package under cold running water for several minutes before adding it to\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to make Pineapple Chicken?\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
