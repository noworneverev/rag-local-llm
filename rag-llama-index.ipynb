{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "# model_path = './models/mistral-7b-instruct-v0.2.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    # model_path='./models/llama-2-13b-chat.Q5_0.gguf',\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,    \n",
    "    completion_to_prompt=completion_to_prompt,        \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here is a poem about fast cars:\n",
      "Racing down the highway, wind in my hair\n",
      "Fast car glides with grace and poise\n",
      "Engine purring smooth, accelerator pressed tight\n",
      "Speeding through the night, feeling alive and bright\n",
      "\n",
      "The thrill of the ride, the rush of the chase\n",
      "Adrenaline coursing through my veins\n",
      "Fast cars take me to places far away\n",
      "Where the world slows down and I feel alive today\n",
      "\n",
      "So here's to fast cars, a symbol of speed\n",
      "A symbol of freedom, a dream to be freed\n",
      "On the open road, where the wind blows free\n",
      "I am one with the car, wild and carefree."
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "\n",
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "text_embedding = embed_model.get_text_embedding(\"hello world\")\n",
    "print(len(text_embedding))\n",
    "\n",
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# # load documents\n",
    "# documents = SimpleDirectoryReader(\n",
    "#     input_files=[\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to print out the response\n",
    "def query(query_str):\n",
    "    streaming_response = query_engine.query(query_str)\n",
    "    streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on the given context, here's an answer to your query:\n",
      "To get started on a personal project in AI, you can follow these steps:\n",
      "1. Identify your interests and goals: Think about what areas of AI interest you the most, such as natural language processing, computer vision, or machine learning. Also, consider what you want to achieve through this project, whether it's to develop a new skill, build a portfolio, or explore a specific application area.\n",
      "2. Research and brainstorm: Once you have a clear idea of your interests and goals, start researching the field by reading articles, watching tutorials, and engaging in online forums. Brainstorm potential project ideas that align with your goals and interests.\n",
      "3. Start small: Don't feel overwhelmed by trying to build a complex project right from the start. Begin with something simple, like building a chatbot or creating a machine learning model to classify images. This will help you gain confidence and develop your skills.\n",
      "4. Join existing projects: Look for open-source AI projects on platforms like GitHub or Kaggle, and see if you can contribute to them. This will give you"
     ]
    }
   ],
   "source": [
    "# answer retrieved from eBook-How-to-Build-a-Career-in-AI.pdf\n",
    "query(\"how do I get started on a personal project in AI?\")\n",
    "# query(\"How do I build a portfolio of AI projects?\")\n",
    "# query(\"Summarize the book in 500 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity is 8.7%."
     ]
    }
   ],
   "source": [
    "# answer retrieved from annualreport.pdf\n",
    "query(\"what was the FY2022 return on equity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To make Pineapple Chicken, you will need the following ingredients:\n",
      "* 1 lb boneless, skinless chicken breasts\n",
      "* 1 cup pineapple juice\n",
      "* 1/4 cup soy sauce\n",
      "* 2 tbsp vegetable oil\n",
      "* 2 tsp garlic powder\n",
      "* 1 tsp crushed red pepper (optional)\n",
      "* 1/4 cup chopped frozen broccoli (thawed)\n",
      "* 1/4 cup pineapple chunks (drained)\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. Heat the oil in a large skillet over medium-high heat. Add all ingredients except for the chicken and cook until heated through, about 5 minutes.\n",
      "2. Add the chicken and cook for another 2-3 minutes, or until it is cooked through.\n",
      "3. Serve over rice or brown rice.\n",
      "Note: If you want to add shrimp, roasted peanuts, beansprouts, or sugar snap peas, you can do so after step 2 and stir-fry them for another"
     ]
    }
   ],
   "source": [
    "# answer retrieved from recipes.pdf\n",
    "query(\"How to make Pineapple Chicken?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Window-sentence retrieval setup (Advanced retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=llm,\n",
    "    save_dir=\"./sentence_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on the provided context, here's how you can build a portfolio of AI projects:\n",
      "1. Start small: As mentioned in the context, don't worry about starting too small. Begin with simple projects that demonstrate your understanding of AI concepts and techniques. This will help you gain confidence and build momentum for more complex projects.\n",
      "2. Focus on practical applications: While it's important to have a solid theoretical foundation in AI, it's equally crucial to show how your skills can be applied in real-world scenarios. Choose projects that demonstrate practical applications of AI, such as image classification, natural language processing, or predictive modeling.\n",
      "3. Show progression: As you progress in your AI journey, add more complex and challenging projects to your portfolio. This will help potential employers see your skill progression and understand your capabilities.\n",
      "4. Communicate your thinking: As mentioned in the context, communication is key when building a portfolio of AI projects. Make sure you can explain your thought process and reasoning behind each project. This will help others understand the value of your work and trust you with larger projects.\n",
      "5. Include a variety of projects:\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I build a portfolio of AI projects?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity (ROE) for Macquarie Group Limited is 18.7%. This can be inferred from the statement in the Remuneration Committee letter that \"the FY2022 full-year dividend is up 32% compared to FY2021, which reflects a strong performance and position for the future.\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what was the FY2022 return on equity?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, here is the step-by-step guide to making Pineapple Chicken:\n",
      "Step 1: Heat oil in a large skillet over medium-high heat. Add all ingredients EXCEPT pineapple and chicken. Cook and stir until heated through, about 5-6 minutes.\n",
      "Step 2: Add pineapple and chicken to the skillet. Cook for another 2 minutes.\n",
      "Serve over instant brown rice or whole wheat pasta.\n",
      "Note: Thaw frozen vegetables in the microwave or by holding the package under cold running water for several minutes.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to make Pineapple Chicken?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Auto-merging retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.node_parser import HierarchicalNodeParser\n",
    "from llama_index.node_parser import get_leaf_nodes\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    merging_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=merging_context,\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    \n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank], service_context=automerging_index.service_context\n",
    "    )\n",
    "    \n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "index = build_automerging_index(\n",
    "    [document],\n",
    "    llm=llm,\n",
    "    save_dir=\"./merging_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_automerging_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building a portfolio of AI projects is an essential step in your career growth as it showcases your skills and progress over time. Here are some steps to help you build a strong portfolio:\n",
      "1. Start small: Don't worry about starting too small. Begin with simple projects that demonstrate your understanding of AI concepts. This will help you gain experience and build your confidence.\n",
      "2. Identify your goals: Think about what you want to achieve with your AI career. What are your long-term goals? What skills do you need to develop to get there? Your portfolio should reflect these goals.\n",
      "3. Choose projects that align with your goals: Select projects that align with your career aspirations and help you develop the skills you need to achieve them.\n",
      "4. Focus on practical experience: While it's important to understand theoretical concepts, your portfolio should primarily showcase practical experience in AI. This means working on real-world projects that demonstrate your ability to apply AI techniques to solve problems.\n",
      "5. Show progression: As you work on more projects, aim to show a progression in complexity and scope. This will demonstrate your growth as an AI practitioner and make your port\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I build a portfolio of AI projects?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, the FY2022 return on equity for Macquarie Group is 18.7%.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what was the FY2022 return on equity?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 381e7c21-cda5-4566-a62e-f287b2ac72b9.\n",
      "...tir\ten.\t\ts\ttext: Add\t\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To make Pineapple Chicken, you will need the following ingredients:\n",
      "* 2 tsp canola or sesame oil\n",
      "* 1 (10-oz) package frozen broccoli (or stir-fry vegetable mix)\n",
      "* 1/4 tsp garlic powder\n",
      "* 1/4 tsp crushed red pepper (optional â€“ use if you like it hot!)\n",
      "* 1 cup coconut milk\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. Add all ingredients except pineapple and chicken to a pot or wok and stir-fry for 5-6 minutes, or until heated through.\n",
      "2. Add pineapple and chicken to the pot and cook for another 2 minutes.\n",
      "3. Stir in coconut milk and simmer for about 10 minutes or until the chicken is cooked through.\n",
      "4. Scatter over the chilli and serve with rice or whole wheat pasta.\n",
      "Note: If using frozen pineapple, thaw it in the microwave or by holding the package under cold running water for several minutes before adding it\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How to make Pineapple Chicken?\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
