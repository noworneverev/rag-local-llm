{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/Coding-Crashkurse/Advanced-RAG/blob/main/code.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    "all_splits = []\n",
    "\n",
    "for file in input_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    all_splits.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Get embedding model\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,        \n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    # n_ctx=2048,\n",
    "    n_ctx=3900,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuances in the question can lead to different results if the question does not capture the embeddings semantically well. MultiQueryRetriever creates variations of the question and thus goes against the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever.get_relevant_documents(\"What was the FY2022 return on equity?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What was the FY2022 return on equity?',\n",
       " 'text': LineList(lines=['Alternative 1: Which companies returned the highest equity returns in FY2022?', '', 'Alternative 2: How did the equity returns of companies in different industries fare in FY2022?', '', 'Alternative 3: What was the correlation between equity returns and revenue growth in FY2022?', '', 'Alternative 4: Which geographic regions had the highest equity returns in FY2022?', '', 'Alternative 5: How did the equity returns of companies with different valuation metrics fare in FY2022?', '    By providing these alternative questions, you hope to help the user explore the database in a more nuanced and targeted manner.'])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"What was the FY2022 return on equity?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Contextual Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Contextual Compression Retriever, you need:\n",
    "\n",
    "- a basic retriever\n",
    "- a document compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the Base Retriever, takes the source documents and forwards them to the Document Compressor. The document compressor takes a list of documents and shortens them by reducing the content of documents or omitting documents altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was the FY2022 return on equity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "FY2022\n",
      "FY2023\n",
      "MGL ordinary shares\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "* FY2022 return on equity\n",
      "* Macquarie begins recognizing an expense for these awards (based on an initial estimate) from 1 April 2021.\n",
      "* The expense is estimated using the price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "* FY2022 return on equity - 18.7%\n",
      "* Prior year (FY2021) return on equity - 14.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "FY2022 return on equity = 18.7%\n",
      "FY2022 earnings per share = $A12.72 (51% on prior year)\n",
      "FY2022 dividends per share = $A6.22 (40% franked)\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021. The expense is estimated using the \n",
      "price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest. In the following financial year,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021. The expense is estimated using the \n",
      "price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest. In the following financial year,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ensemble Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm.\n",
    "\n",
    "By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single algorithm.\n",
    "\n",
    "The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as “hybrid search”. The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(all_splits)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "chroma_vectorstore = Chroma.from_documents(all_splits, embeddings)\n",
    "chroma_retriever = chroma_vectorstore.as_retriever()\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \\nbegins recognising an expense for these awards (based on an initial estimate) from 1\\xa0April 2021. The expense is estimated using the \\nprice of MGL ordinary shares as at 31\\xa0March 2022 and the number of equity awards expected to vest. In the following financial year,', metadata={'page': 135, 'source': './docs/annualreport.pdf'}),\n",
       " Document(page_content='14FY2022 net profit\\n$A4,706 m\\n  56% on prior year\\nFY2022 net operating income\\n$A17,324 m\\n  36% on prior yearFY2022 operating expenses\\n$A10,785 m\\n  22% on prior year\\nFY2022 earnings per share\\n$A12.72\\n  51% on prior yearFY2022 return on equity\\n18.7%\\n  from 14.3% in prior year\\nFY2022 dividends per share\\n$A6.22\\n (40% franked)\\n  32% on prior yearFY2022 effective tax rate\\n25.2%\\n   from 23.0%  \\nin prior yearAssets under management\\n$A774.8b\\n   from $A563.5b  \\nas at 31\\xa0March 2021Financial Highlights', metadata={'page': 15, 'source': './docs/annualreport.pdf'}),\n",
       " Document(page_content='parent facilities, and return-to-work coaching for parents and \\ntheir\\xa0managers. \\nIn ANZ, Macquarie was accredited as a Family Friendly Workplace \\nand received a National Level 2 carer accreditation from \\nCarers+Employers, recognised for our deep commitment to \\nsupporting the careers of carers. \\nMacquarie’s global return to work rate was 96% in FY2022 and high \\nretention rates continue to be achieved for staff who have taken \\nparental leave.\\nCultural diversity and racial equity', metadata={'source': './docs/annualreport.pdf', 'page': 43}),\n",
       " Document(page_content='learn how much time most machine learning engineers spend iteratively cleaning datasets.✓\\n✓\\nWhat do you do in a typical week or day?\\nWhat are the most important tasks in this role?\\nWhat skills are most important for success?\\nHow does your team work together to accomplish its goals?\\nWhat is the hiring process?\\nConsidering candidates who stood out in the past, what enabled them to shine?✓\\n✓\\n✓\\n✓\\n✓\\n✓Using Informational Interviews to Find the Right Job CHAPTER 8', metadata={'source': './docs/eBook-How-to-Build-a-Career-in-AI.pdf', 'page': 28})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(query=question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
