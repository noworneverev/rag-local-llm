{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/Coding-Crashkurse/Advanced-RAG/blob/main/code.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\"./docs/eBook-How-to-Build-a-Career-in-AI.pdf\", \"./docs/recipes.pdf\", \"./docs/annualreport.pdf\"]\n",
    "all_splits = []\n",
    "\n",
    "for file in input_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    all_splits.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Get embedding model\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,        \n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    # n_ctx=2048,\n",
    "    n_ctx=3900,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuances in the question can lead to different results if the question does not capture the embeddings semantically well. MultiQueryRetriever creates variations of the question and thus goes against the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever.get_relevant_documents(\"What was the FY2022 return on equity?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What was the FY2022 return on equity?',\n",
       " 'text': LineList(lines=['Alternative 1: Which companies returned the highest equity returns in FY2022?', '', 'Alternative 2: How did the equity returns of companies in different industries fare in FY2022?', '', 'Alternative 3: What was the correlation between equity returns and revenue growth in FY2022?', '', 'Alternative 4: Which geographic regions had the highest equity returns in FY2022?', '', 'Alternative 5: How did the equity returns of companies with different valuation metrics fare in FY2022?', '    By providing these alternative questions, you hope to help the user explore the database in a more nuanced and targeted manner.'])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"What was the FY2022 return on equity?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Contextual Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Contextual Compression Retriever, you need:\n",
    "\n",
    "- a basic retriever\n",
    "- a document compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the Base Retriever, takes the source documents and forwards them to the Document Compressor. The document compressor takes a list of documents and shortens them by reducing the content of documents or omitting documents altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was the FY2022 return on equity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "c:\\Hiwi_Project\\langchain-local-model\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "FY2022\n",
      "FY2023\n",
      "MGL ordinary shares\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "* FY2022 return on equity\n",
      "* Macquarie begins recognizing an expense for these awards (based on an initial estimate) from 1 April 2021.\n",
      "* The expense is estimated using the price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "* FY2022 return on equity - 18.7%\n",
      "* Prior year (FY2021) return on equity - 14.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "FY2022 return on equity = 18.7%\n",
      "FY2022 earnings per share = $A12.72 (51% on prior year)\n",
      "FY2022 dividends per share = $A6.22 (40% franked)\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021. The expense is estimated using the \n",
      "price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest. In the following financial year,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021. The expense is estimated using the \n",
      "price of MGL ordinary shares as at 31 March 2022 and the number of equity awards expected to vest. In the following financial year,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "have been previously disclosed. Equity awards in respect of FY2022 performance will be granted during FY2023; however, Macquarie \n",
      "begins recognising an expense for these awards (based on an initial estimate) from 1 April 2021\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "14FY2022 net profit\n",
      "$A4,706 m\n",
      "  56% on prior year\n",
      "FY2022 net operating income\n",
      "$A17,324 m\n",
      "  36% on prior yearFY2022 operating expenses\n",
      "$A10,785 m\n",
      "  22% on prior year\n",
      "FY2022 earnings per share\n",
      "$A12.72\n",
      "  51% on prior yearFY2022 return on equity\n",
      "18.7%\n",
      "  from 14.3% in prior year\n",
      "FY2022 dividends per share\n",
      "$A6.22\n",
      " (40% franked)\n",
      "  32% on prior yearFY2022 effective tax rate\n",
      "25.2%\n",
      "   from 23.0%  \n",
      "in prior yearAssets under management\n",
      "$A774.8b\n",
      "   from $A563.5b  \n",
      "as at 31 March 2021Financial Highlights\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
